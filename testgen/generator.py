import ast
from pathlib import Path

from .llm_router import generate
from .openai_client import DEFAULT_MODEL
from .prompt import build_prompt
from .reader import read_criterion

ROOT_DIR = Path(__file__).resolve().parents[1]
OUTPUT_PATH = ROOT_DIR / "tests" / "test_generated.py"


def strip_code_fence(text: str) -> str:
    t = text.strip()
    if t.startswith("```") and t.count("\n") > 0:
        parts = t.split("```")
    if len(parts) >= 3:
        middle = max(parts[1:-1], key=len)
        return middle.strip()
    if t.startswith("```python"):
        return t[len("```python"):].strip().rstrip("```")
    return t


def is_valid_python(content: str) -> bool:
    try:
        ast.parse(content)
        return True
    except SyntaxError:
        return False


def write_output_file(content: str):
    d = OUTPUT_PATH.parent
    d.mkdir(parents=True, exist_ok=True)
    header = "# GENERATED BY testgen - do not edit\n"
    if not content.startswith(header):
        content = header + content
    OUTPUT_PATH.write_text(content, encoding="utf-8")


def orchestrate(criterion_file: str, model: str = None):
    crit = read_criterion(criterion_file)
    prompt_block = build_prompt(crit, target_framework="pytest")
    print(f"Sending prompt to {model} (system message trimmed):")
    print(prompt_block['system'][:200] + ("..." if len(prompt_block['system']) > 200 else ""))

    model_to_use = model or DEFAULT_MODEL
    response_text = generate(prompt_block, model=model_to_use)

    cleaned = strip_code_fence(response_text)

    if not is_valid_python(cleaned):
        print("Warning: generated code has syntax errors. Writing anyway for inspection.")
        write_output_file(cleaned)
        return

    write_output_file(cleaned)
    print(f"Wrote generated tests to {OUTPUT_PATH}")
